---
title: "Turbulent code in water models"
subtitle: "R code"
author: "Arnald Puy"
header-includes:
  - \usepackage[font=footnotesize]{caption}
  - \usepackage{dirtytalk}
  - \usepackage{booktabs}
  - \usepackage{tabulary}
  - \usepackage{enumitem}
  - \usepackage{lmodern}
  - \usepackage[T1]{fontenc}
  - \usepackage{tikz}
output:
  pdf_document:
    fig_caption: yes
    number_sections: yes
    toc: yes
    toc_depth: 2
    keep_tex: true
  word_document:
    toc: no
    toc_depth: '2'
  html_document:
    keep_md: true
link-citations: yes
fontsize: 11pt
bibliography: /Users/arnaldpuy/Documents/bibtex/ERC-sensitivity.bib
---

```{r setup, include=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE, dev = "tikz", cache = TRUE)
```

```{r preliminary, warning=FALSE}

# PRELIMINARY FUNCTIONS ########################################################

# Load the packages
sensobol::load_packages(c("data.table", "tidyverse", "openxlsx", "tm", "stringr", 
                          "pdftools", "tidytext", "scales", "cowplot",
                          "ggrepel", "tidyquant", "text2vec"))

# Create custom theme
# Create custom theme
theme_AP <- function() {
  theme_bw() +
    theme(panel.grid.major = element_blank(),
          panel.grid.minor = element_blank(),
          legend.background = element_rect(fill = "transparent", color = NA),
          legend.key = element_rect(fill = "transparent", color = NA), 
          strip.background = element_rect(fill = "white"), 
          legend.margin = margin(0.5, 0.1, 0.1, 0.1),
          legend.box.margin = margin(0.2,-4,-7,-7), 
          plot.margin = margin(3, 4, 0, 4), 
          legend.text = element_text(size = 6), 
          axis.title = element_text(size = 10),
          axis.text.x = element_text(size = 7),
          axis.text.y = element_text(size = 7),
          axis.title.x = element_text(size = 7.3),
          axis.title.y = element_text(size = 7.3),
          strip.text.x = element_text(size = 7.4),
          strip.text.y = element_text(size = 7.4),
          legend.key.width = unit(0.4, "cm"), 
          legend.key.height = unit(0.5, "lines"), 
          legend.title = element_text(size = 7.5)) 
}
```

```{r tests}

# FUNCTIONS TO CLEAN THE TEXT ##################################################

# Function to remove words from text
removeWords <- function(str, stopwords) {
  x <- unlist(strsplit(str, " "))
  paste(x[!x %in% stopwords], collapse = " ")
}

# Function to remove punctuation, citations, numbers, stopwords in english, 
# bring to lowercase and strip whitespace, and especial characters, etc...
clear_text <- function(x, stem = TRUE) {
  
  y <- tolower(x)
  y <- str_replace_all(y, "[[:punct:]]", " ") # Remove punctuation characters
  y <- tm::removeNumbers(y)
  y <- tm::removeWords(y, stopwords::stopwords(language = "en"))
  y <- str_remove_all(y, "[^[\\da-zA-Z ]]")# Remove all non-alphanumerical
  y <- gsub("\\s[A-Za-z](?= )", " ", y, perl = TRUE) # Remove isolated letters
  #y <- tm::stripWhitespace(y)
  y <- str_squish(y)
  
  if (stem == TRUE) {
    y <- stemDocument(y) # Stem the document and keep only the root of the word
  }
  
  return(y)
}

# READ DATA ####################################################################

dt <- data.table(read.xlsx("final.dt.xlsx"))
dt[, keywords.large:= tolower(keywords.large)]
dt[, abstract.cleaned:= clear_text(abstract.large, stem = FALSE)]

# TERMS TO SEARCH ##############################################################

keywords <- c("validation", "verification", "calibration", "confirmation", "evaluation", 
              "benchmarking")
keywords.stemmed <- stemDocument(keywords)

# keywords.stemmed <- c(
#  "calibration", "verification", "validation", "evaluation",
#  "calibrations", "verifications", "validations", "evaluations",  
#  "calibrated", "verified", "validated", "evaluated",             
#  "calibrating", "verifying", "validating", "evaluating",        
#  "calibrative", "verificative", "validative", "evaluative",    
#  "calibratively", "verificatively", "validatively", "evaluatively"  
# )


# SEARCHING FOR TERMS ##########################################################

# Check which papers include keywords in abstract, keywords or title -----------
selected_cols <- c("title", "abstract", "keywords")
#selected_cols <- c("title.large", "abstract.large", "keywords.large")
out <- list()

for(i in 1:length(keywords.stemmed)) {
  
  out[[i]] <- dt[, lapply(.SD, function(x) 
    str_detect(x, keywords.stemmed[i])), .SDcols = (selected_cols)]
  
}

# ARRANGE DATA #################################################################

names(out) <- keywords.stemmed

valid.dt <- lapply(out, function(x) rowSums(x, na.rm = TRUE) > 0L) %>%
  do.call(cbind, .) %>%
  data.table() %>%
  .[, any.column:= rowSums(.SD) > 0]

full.dt <- cbind(dt, valid.dt)
full.dt.cols <- data.frame(full.dt[, .SD, .SDcols = keywords.stemmed])

# DESCRIPTIVE STATISTICS #######################################################

full.dt[, lapply(.SD, sum), .SDcols = keywords.stemmed]

# Count number of papers with mentions to validation, validation + verification,
# validation + calibration, etc ------------------------------------------------

# Function to count the number of TRUE values shared between columns
count_shared_true <- function(data, cols) {
  sum(rowSums(data[, cols]) == length(cols))
}

# Create an empty list to store results
results_list <- list()

# Loop through all combinations of columns and count shared TRUE values
for (size in 2:length(keywords.stemmed)) {
  
  for (cols_combination in combn(ncol(full.dt.cols), size, simplify = FALSE)) {
    
    shared_true_count <- count_shared_true(data = full.dt.cols, cols = cols_combination)
    col_names <- colnames(full.dt.cols)[cols_combination]
    
    # Append results to the list
    results_list <- c(results_list, list(data.table(combination = paste(col_names, collapse = ", "), N = shared_true_count)))
  }
}

# Combine the list of data.tables into a single data.table
comb_dt <- rbindlist(results_list)
```

```{r plots_descriptive, dependson="tests", warning=FALSE}

# PLOTS ########################################################################

plot.year <- full.dt[, .N, year] %>%
  .[!year == 2023] %>%
  ggplot(., aes(year, N)) +
  geom_line() +
  labs(x = "Year", y = "Nº papers") +
  theme_AP()

plot.year

plot.model.year <- full.dt[, .N, .(Model, year)] %>%
  .[!year == 2023] %>%
  ggplot(., aes(year, N, color = Model)) +
  geom_ma(ma_fun = SMA, n = 3, lty = 1) +
  labs(x = "Year", y = "") +
  theme_AP() 

plot.model.year

plot.keywords.time <- merge(full.dt, full.dt[, .(total.papers = .N), year], by = "year") %>%
  melt(., measure.vars = keywords.stemmed) %>%
  .[, sum(value, na.rm = TRUE), .(variable, year, total.papers)] %>%
  .[, fraction:= V1 / total.papers] %>%
  ggplot(., aes(year, fraction, color = variable)) +
  scale_color_discrete(name = "") +
  geom_ma(ma_fun = SMA, n = 5, lty = 1) +
  theme_AP() +
  theme(legend.position = c(0.2, 0.8)) +
  labs(x = "Year", y = "Fraction papers")

plot.keywords.time

tmp <- merge(full.dt, full.dt[, .(total.papers = .N), Model], by = "Model") %>%
  melt(., measure.vars = keywords.stemmed) %>%
  .[, sum(value, na.rm = TRUE), .(variable, Model, total.papers)] %>%
  .[, fraction:= V1 / total.papers] %>%
  mutate(variable = as.factor(variable),
       name = reorder_within(Model, fraction, variable))

plot.keyword.per.model <- tmp %>%
  ggplot(., aes(name, fraction)) +
  geom_bar(stat = "identity") +
  coord_flip() + 
  facet_wrap(~variable, scales = "free") + 
  scale_x_reordered() +
  scale_y_continuous(breaks = pretty_breaks(n = 3)) +
  theme_AP() +
  labs(x = "", y = "Fraction papers") + 
  theme(axis.text.y = element_text(size = 6.5))

plot.keyword.per.model

plot.keyword.comb <- comb_dt[N > 10] %>%
  ggplot(., aes(reorder(combination, N), N)) +
  geom_bar(stat = "identity") + 
  coord_flip() +
  theme_AP() + 
  theme(axis.text.y = element_text(size = 7)) +
  labs(x = "", y = "Nº papers")

plot.keyword.comb 

total.papers.model <- full.dt[, .N, Model]

plot.valid.calibr <- tmp %>%
  dcast(., Model ~ variable, value.var = "fraction") %>%
  merge(., total.papers.model, by = "Model") %>%
  ggplot(., aes(valid, calibr, label = Model)) +
  geom_point(color = "grey") +
  geom_text_repel(size = 1.5) + 
  theme_AP() + 
  theme(legend.position = "none")

plot.valid.calibr

plot.evalu.valid <- tmp %>%
  dcast(., Model ~ variable, value.var = "fraction") %>%
  merge(., total.papers.model, by = "Model") %>%
  ggplot(., aes(evalu, valid, label = Model)) +
  geom_point(color = "grey") +
  geom_text_repel(size = 1.5) + 
  theme_AP() + 
  theme(legend.position = "right")

plot.evalu.valid
```

```{r plot.time, dependson="plots_descriptive", fig.height=2.5, fig.width=3}

plot.keywords.time

```


```{r merge_plots_descriptive, dependson="plots_descriptive", fig.width=5.5, fig.height = 6, warning=FALSE, dev="pdf"}

# MERGE DESCRIPTIVE PLOTS #####################################################

top <- plot_grid(plot.keywords.time, plot.keyword.comb, ncol = 2, labels = "auto", 
                 rel_widths = c(0.45, 0.55))

plot.merged <- plot_grid(top, plot.keyword.per.model, ncol = 1, 
                         labels = c("", "c"), rel_heights = c(0.3, 0.7))

plot.merged

```

```{r merge_plots_descriptive2, dependson="plots_descriptive", fig.width=5.5, fig.height = 8.5, warning=FALSE, dev="pdf"}

toppest <- plot_grid(plot.year, plot.model.year, ncol = 2, labels = "auto", 
                     rel_widths = c(0.4, 0.6))

top <- plot_grid(plot.keywords.time, plot.keyword.comb, ncol = 2, labels = "auto", 
                 rel_widths = c(0.45, 0.55))

both.top <- plot_grid(toppest, top, ncol = 1, rel_heights = c(0.4, 0.6))

plot.merged <- plot_grid(both.top, plot.keyword.per.model, ncol = 1, 
                         labels = c("", "c"), rel_heights = c(0.4, 0.6))

plot.merged
```

```{r plot.years, dependson="plots_descriptive", fig.height=2.35, fig.width=5.5, dev="pdf"}

toppest
```

```{r scatter.plots, dependson=c("merge_plots_descriptive", "plots_descriptive"), fig.height=4, fig.width=5.5, dev="pdf"}

scatter.plots <- plot_grid(plot.valid.calibr, plot.evalu.valid, ncol = 2, 
                           labels = c("c", "d"), rel_widths = c(0.46, 0.54))

plot_grid(top, scatter.plots, ncol = 1, rel_heights = c(0.5, 0.5))
```

```{r token_function}

# TOKEN ANALYSIS ################################################################

# Create function --------------------------------------------------------------
tokenize_fun <- function(dt, word, keywords, N.tokens) {
  
  # Create long dataset
  dt <- melt(dt, measure.vars = keywords)
  output <- dt[variable == word & value == TRUE]
  
  # Token analysis ------------------------------
  # We count the co-occurences of words without taking into account their order
  # within the n-token
  token.analysis <- output %>%
    unnest_tokens(bigram, abstract.cleaned, token = "ngrams", n = N.tokens) %>%
    separate(bigram, into = c("word1", "word2"), sep = " ") %>%
    data.table() %>%
    .[, `:=`(word1= pmin(word1, word2), word2 = pmax(word1, word2))] %>%
    count(word1, word2, sort = TRUE) %>%
    unite(., col = "bigram", c("word1", "word2"), sep = " ") %>%
    data.table()
  
  # Vector to retrieve only the bigrams with uncertainti or sensit 
  vec <- token.analysis[, str_detect(bigram, word)]
  
  # Final dataset
  output.dt <- token.analysis[vec]
  
  # Plot the q0 words most commonly 
  # associated with uncertainti and sensit ------
  plot.token <-  output.dt %>%
    .[, sum(n), bigram] %>%
    .[order(-V1)] %>%
    .[, head(.SD, 10)] %>%
    ggplot(., aes(reorder(bigram, V1, sum), V1)) +
    geom_bar(stat = "identity") +
    coord_flip() +
    scale_y_continuous(breaks = pretty_breaks(n = 3)) +
    theme_AP() +
    labs(y = "n", x = "") +
    ggtitle(word) +
    theme(legend.position = "none", 
          plot.title = element_text(size = 9), 
          axis.text.y = element_text(size = 7))
  
  # Arrange and output --------------------------
  
  out <- list(output.dt, plot.token)
  names(out) <- c("data", "token")
  
  return(out)
  
}
```

```{r token_analysis, dependson=c("token_function", "tests")}

# RUN TOKEN ANALYSIS ###########################################################

N.tokens <- 2
token.dt <- list()

for (j in keywords.stemmed) {
  
  token.dt[[j]] <- tokenize_fun(dt = full.dt, word = j, 
                                keywords = keywords.stemmed, 
                                N.tokens = N.tokens)
  
}
```

```{r plot_merged_token, dependson=c("token_analysis", "merge_plots_descriptive"), fig.height=8, warning=FALSE, fig.width=6, dev="pdf"}

# PLOT RESULTS #################################################################

plot.tokens <- plot_grid(token.dt$valid[[2]], token.dt$verif[[2]], token.dt$calibr[[2]], 
                         token.dt$confirm[[2]], token.dt$evalu[[2]], token.dt$benchmark[[2]], 
                         ncol = 3)

plot_grid(plot.merged, plot.tokens, ncol = 1, labels = c("", "d"), 
          rel_heights = c(0.57, 0.43))

```

```{r plot_tokens, dependson="plot_merged_token", fig.height=3.5, fig.width=6, dev="pdf"}

plot.tokens
```

# Close reading

```{r close_reading, dev = "pdf"}

# KEYWORDS TO LOOK FOR #########################################################

keywords_selected <- c(
  "benchmark", "calibrate", "confirm", "evaluate", "validate", "verify",
  "benchmarks", "calibrates", "confirms", "evaluates", "validates", "verifies",
  "benchmarked", "calibrated", "confirmed", "evaluated", "validated", "verified",
  "benchmarking", "calibrating", "confirming", "evaluating", "validating", "verifying",
  "calibration", "confirmation", "evaluation", "validation", "verification",
  "calibrations", "confirmations", "evaluations", "validations", "verifications",
  "benchmarkable", "calibrative", "confirmative", "evaluative", "validative", "verificative",
  "calibratively", "confirmatively", "evaluatively", "validatively", "verificatively"
)

keywords_selected_stemmed <- unique(stemDocument(keywords_selected))[-c(1, 3, 6)]

# READ IN DATA #################################################################

list.close.reading <- data.table(read.xlsx("dt.papers.close.reading.xlsx"))

dt.students <- data.table(read.xlsx("validation_work_students.xlsx")) %>%
  .[, title.large:= tolower(title)]

dt.close.reading <- merge(list.close.reading[, .(Model, title.large)], 
      dt.students[, .(doi, title.large, paragraph)], by = "title.large") %>%
  .[, title.large:= tolower(title.large)] %>%
  .[, paragraph.clean:= clear_text(paragraph)]

total.models <- dt.close.reading[, .N, .(Model, title.large)] %>%
  .[, .(Model)] %>%
  .[, .(total.papers = .N), Model]

# COUNT KEYWORDS ###############################################################

out <- out.stemmed <- list()

for(i in 1:length(keywords_selected)) {

  pattern <- paste0("\\b", keywords_selected[i], "\\b")
  
  out[[i]] <- dt.close.reading[, lapply(.SD, function(x) 
    str_count(x, pattern)), .SDcols = "paragraph"]
  
}

for(i in 1:length(keywords_selected_stemmed)) {

  
  out.stemmed[[i]] <- dt.close.reading[, lapply(.SD, function(x) 
    str_count(x, keywords_selected_stemmed[i])), .SDcols = "paragraph"]
  
}

# ARRANGE DATA #################################################################

dt.keywords <- do.call(cbind, out) %>%
  data.table() 

dt.keywords.stemmed <- do.call(cbind, out.stemmed) %>%
  data.table() 

colnames(dt.keywords) <- keywords_selected
colnames(dt.keywords.stemmed) <- keywords_selected_stemmed

full.dt.keywords <- cbind(dt.keywords, dt.keywords.stemmed)

vec.columns <- colSums(full.dt.keywords, na.rm = TRUE)

colnames.keywords <- names(vec.columns[!vec.columns == 0])

full.dt.close.reading <- cbind(dt.close.reading, full.dt.keywords) %>%
  merge(., total.models, by = "Model")


# COSINE ANALYSIS ##############################################################

clean_text <- melt(full.dt.close.reading, 
                   measure.vars = keywords.stemmed) %>%
  .[!value == 0] %>%
  unnest_tokens(word, paragraph.clean) %>%
  anti_join(stop_words) %>%
  group_by(variable) %>%
  summarize(paragraph.clean = paste(word, collapse = " "))


# Iterate over the text --------------------------------------------------------

it <- itoken(clean_text$paragraph.clean, progressbar = FALSE)

# Create vocabulary ------------------------------------------------------------

vocab <- create_vocabulary(it)

# Vectorize text ---------------------------------------------------------------

vectorizer <- vocab_vectorizer(vocab)
dtm <- create_dtm(it, vectorizer)

# Apply TF-IDF transformation --------------------------------------------------

tfidf <- TfIdf$new()
dtm_tfidf <- tfidf$fit_transform(dtm)

# Compute cosine similarity matrix ---------------------------------------------

similarity_matrix <- sim2(dtm_tfidf, method = "cosine", norm = "l2")

# Convert the similarity matrix into a tidy format_-----------------------------

similarity_df <- as.data.frame(as.table(as.matrix(similarity_matrix)))
colnames(similarity_df) <- c("Paragraph1", "Paragraph2", "Similarity")

# Filter for term-specific comparisons if needed -------------------------------

similarity_df <- similarity_df %>%
  filter(Paragraph1 != Paragraph2) %>%
  arrange(desc(Similarity))

# PLOT #########################################################################

heatmap_data <- similarity_matrix
rownames(heatmap_data) <- clean_text$variable
colnames(heatmap_data) <- clean_text$variable
```

```{r plot.heatmap, dependson="close_reading", fig.height=2.5, fig.width=2.5, dev="pdf"}


plot.heatmap <- pheatmap::pheatmap(as.matrix(heatmap_data), 
              color = colorRampPalette(c("white", "blue"))(50), 
              fontsize_row = 6, 
              fontsize_col = 6, 
              treeheight_row = 10,  
              treeheight_col = 10, 
              legend = FALSE)


plot.heatmap

grob <- plot.heatmap$gtable
```


```{r merge.plots.heatmap, dependson=c("merge_plots_descriptive", "plots_descriptive", "plot.heatmap"), fig.height=3.7, fig.width=5.5, dev="pdf"}

scatter.plots <- plot_grid(ggdraw(grob), plot.valid.calibr, plot.evalu.valid, ncol = 3, 
                           labels = c("c", "d", "e"), rel_widths = c(0.33, 0.33, 0.33))

plot_grid(top, scatter.plots, ncol = 1, rel_heights = c(0.5, 0.5))
```

\newpage

```{r microbenchmark}

# SESSION INFORMATION ##########################################################

sessionInfo()

## Return the machine CPU
cat("Machine:     "); print(get_cpu()$model_name)

## Return number of true cores
cat("Num cores:   "); print(detectCores(logical = FALSE))

## Return number of threads
cat("Num threads: "); print(detectCores(logical = FALSE))
```
